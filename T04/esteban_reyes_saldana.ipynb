{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"esteban_reyes_saldana.ipynb","provenance":[],"collapsed_sections":["M7bdgkEjcX5r","Rba8ReFe4JOL"],"mount_file_id":"1aj_zPhQmgt2nhvQuyb8QoUOtwoNL8vAy","authorship_tag":"ABX9TyNAHwDSWDcxFGI6ajXyXhmm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pZ7X6IZfMBmb"},"source":["# Tarea 4\n","\n","## Esteban Reyes Saldaña\n","\n","### Modelos de Lenguaje Estadísticos"]},{"cell_type":"markdown","metadata":{"id":"FZU9XljAMGjA"},"source":["## 2 El Ahorcado"]},{"cell_type":"markdown","metadata":{"id":"7Ow-WnEwMJHc"},"source":["Diseñe una función que sea capaz de encontrar los caracteres faltantes de una palabra. Para ello proponga una adaptación simple de la estrategia de corrección ortográfica propuesta por\n","Norvig. La función de el ahorcado debe poder tratar con hasta 4 caracteres desconocidos en palabras de longitud arbitraria. La función debe trabajar en tiempo razonable (≈ 1 minuto en una laptop)."]},{"cell_type":"code","metadata":{"id":"AUassAmdbRZ1"},"source":["# Corpus \n","target_url = 'http://norvig.com/big.txt'\n","from urllib.request import urlopen\n"," \n","big = urlopen(target_url).read().decode('utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3YsHTIfai4P"},"source":["import re\n","from collections import Counter\n","\n","# Codigo Norvig\n","def words(text): return re.findall(r'\\w+', text.lower())\n"," \n","WORDS = Counter(words(big))\n"," \n","def P(word, N=sum(WORDS.values())): \n","    \"Probability of `word`.\"\n","    return WORDS[word] / N\n"," \n","def correction(word):\n","    \"Most probable spelling correction for word.\"\n","    return max(candidates(word), key=P)\n"," \n","def candidates(word): \n","    \"Generate possible spelling corrections for word.\"\n","    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n"," \n","def known(words): \n","    \"The subset of `words` that appear in the dictionary of WORDS.\"\n","    return set(w for w in words if w in WORDS)\n"," \n","def edits1(word):\n","    # Quito delets porque quiero una longitud fija\n","    # Quito transposiciones porque el orden es correcto\n","    \"All edits that are one edit away from `word`.\"    \n","    letters    = 'abcdefghijklmnopqrstuvwxyz'\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","    inserts    = [L + c + R               for L, R in splits for c in letters]\n","    return set(replaces + inserts)\n"," \n","def edits2(word): \n","    \"All edits that are two edits away from `word`.\"\n","    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmZSa_2BBGKP"},"source":["def hangman3(word3, indexes, level = 3) :\n","  letters    = 'abcdefghijklmnopqrstuvwxyz'\n","  # Creo todos los casos de dos guiones remplazando primer guión\n","  # por letra del abecedario y resulevo\n","  if level == 3 :\n","    posibles = [word3[:indexes[0]] + l + word3[indexes[0]+ 1:] for l in letters]\n","  if level == 4 :\n","    posibles = [word3[:indexes[1]] + l + word3[indexes[1]+ 1:] for l in letters]\n","\n","  # Busco corrección\n","  c = [correction(w) for w in posibles]\n","  # Me quedo con las que no tengan guiones\n","  c = [w for w in c if w.find('_') == -1]\n","  # Si no encuentro palabra, regreso\n","  if len(c) == 0 :\n","    return word3\n","  else :\n","    # Regreso palabras\n","    return c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uMP4ZmPIqGG"},"source":["def choose_word(c) :\n","  return max(c, key=P)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3Pun6znakaV"},"source":["def hangman(word) :\n","  letters    = 'abcdefghijklmnopqrstuvwxyz'\n","  # Busco posiciones de guion\n","  indexes = [x.start() for x in re.finditer('_', word)]\n","\n","  # Si solo me faltan dos letras, ya lo puedo hacer\n","  if len(indexes) <= 2 :\n","    return correction(word)\n","  elif  len(indexes) == 3 :\n","    # Paso caso de tres a un caso de dos guiones\n","    c = hangman3(word, indexes)\n","    # Si no encuentro \n","    # Elijo de la de mayor probabilidad\n","    return choose_word(c)\n","  elif len(indexes) == 4 :\n","    for letter in letters :\n","      # Lo llevo al caso de tres que resolví previamente\n","      word3 = word[:indexes[0]] + letter + word[indexes[0]+1: ]\n","      c     = hangman3(word3, indexes, level = 4)\n","      # Muestra primera coincidencia\n","      if len(c) > 0 :\n","        return choose_word(c)\n","        break\n","  else :\n","    print('Only four unknown letters allow.')\n","    return \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"V5qx94JY97e6","executionInfo":{"status":"ok","timestamp":1616134612543,"user_tz":360,"elapsed":326,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"434b9a6c-45c5-4300-e68b-723cb3641d2d"},"source":["# 1\n","hangman('pe_p_e')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'people'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"TVwcpRNRnJb_","executionInfo":{"status":"ok","timestamp":1615682772771,"user_tz":360,"elapsed":4773,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"6b75f2f2-f36c-42b3-aee1-e17e0d48f90e"},"source":["# 2\n","hangman('phi__sop_y')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'philosophy'"]},"metadata":{"tags":[]},"execution_count":361}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NuVmrI8YK6D4","executionInfo":{"status":"ok","timestamp":1615682782162,"user_tz":360,"elapsed":4778,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"416905b7-7558-4846-fde1-6dcfd9b1d699"},"source":["# Midiendo tiempo\n","tic = time.time()\n","hangman('phi__sop_y')\n","print('time: ', time.time() - tic, 'seconds.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["time:  4.3547046184539795 seconds.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"T5RNxAfiLQEs","executionInfo":{"status":"ok","timestamp":1615682827875,"user_tz":360,"elapsed":42792,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"d00842e9-6f27-4051-f4bb-3d22b3c1fa22"},"source":["# 3\n","hangman('si_nif_c_nc_')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'significance'"]},"metadata":{"tags":[]},"execution_count":363}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gbCCq9HCdrBt","executionInfo":{"status":"ok","timestamp":1615682498414,"user_tz":360,"elapsed":42892,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"2de8f7bb-3a20-4185-8cb4-05a240154a28"},"source":["# Midiendo tiempo\n","tic = time.time()\n","hangman('si_nif_c_nc_')\n","print('time: ', time.time() - tic, 'seconds.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["time:  42.21551513671875 seconds.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tCeM5qoZULTA","executionInfo":{"status":"ok","timestamp":1615685707892,"user_tz":360,"elapsed":78602,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"55370f6b-ccce-4ceb-b12a-0ba4e14685b9"},"source":["words = ['h_ll_'   , 'c__t_y', 's_p__', 'com____r', 'la_g_a_e', \n","         'la_g_a__', 'n__l'  , '_or_d', 'n_t__a_' , 'e__', '_o__']\n","i = 1\n","for w in words: \n","  z = hangman(w)\n","  print(f\"{i} : {w:<8s} [{len(w)}] #=> {z:8s} [{len(z)}]\")\n","  i += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 : h_ll_    [5] #=> hills   [5]\n","2 : c__t_y   [6] #=> costly  [6]\n","3 : s_p__    [5] #=> supra   [5]\n","4 : com____r [8] #=> commoner[8]\n","5 : la_g_a_e [8] #=> language[8]\n","6 : la_g_a__ [8] #=> language[8]\n","7 : n__l     [4] #=> nail    [4]\n","8 : _or_d    [5] #=> world   [5]\n","9 : n_t__a_  [7] #=> natural [7]\n","10 : e__      [3] #=> end     [3]\n","11 : _o__     [4] #=> more    [4]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DiCuTfn0ZsbP"},"source":["## 3 Modelos del Lenguaje"]},{"cell_type":"markdown","metadata":{"id":"10bqNSbAZzDU"},"source":["## 3.1 Construcción de Modelo"]},{"cell_type":"markdown","metadata":{"id":"CfEoNO4fZ1bQ"},"source":["1. Construya un corpus para entrenar un modelo de lenguaje. Para ello use la parte de libros gutenberg del archivo big.txt, y combinelo con los datos de wikitex-2 o wikitex-103, según le convenga en tiempo y hardware. Defina su vocabulario y enmascare con \\<unk> toda palabra que no esté en su vocabulari (los wikitex ya lo tienen, pero tal vez necesite acotarlo más). Valore si conviene usar el tokenizador de oraciones de nltk, o usar el texto como una oración gigante (mencioné cuál utilizó y por qué). Para este ejercicio podría no ser necesario denir tokens especiales \\<s> y \\</s> dependiendo de si tokenizo oraciones.\n","Limpie el texto dejando solo palabras en minúscula, quitando puntuación, números, etc."]},{"cell_type":"markdown","metadata":{"id":"8Z6v0iOusHT1"},"source":["**Nota**. Utilicé un tokenizador por oraciones dado que de esa forma se definió en el libro de Dan y entonces sirvió como guía para desarrolar el modelo del lenguaje paso a paso.\n","\n","Limpié dichas oraciones con una expresión regular, dado que el texto tenía muchos simbolos raros inlcuso de otros idiomas."]},{"cell_type":"code","metadata":{"id":"OSdaGauOZiln"},"source":["# Librerías\n","import os\n","import re\n","import math\n","import nltk \n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ycLqKlik54FN"},"source":["from nltk.tokenize import TweetTokenizer\n","tknzr = TweetTokenizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNRGaxwwqvBF"},"source":["def get_texts_from_file (path_corpus, path_truth) :\n","  tr_txt = []\n","  tr_y   = []\n","\n","  with open (path_corpus, \"r\") as f_corpus, open(path_truth, \"r\", encoding='utf-8') as f_truth:\n","    for twitt in f_corpus:\n","      twitt = twitt.replace('\\n', ' ')\n","      if twitt == ' ' or twitt == '  ':\n","        continue\n","      else :\n","        tr_txt += [twitt]\n","    for label in f_truth:\n","      if label != ' \\n' :\n","        tr_y   += [label]\n","  \n","\n","  return tr_txt, tr_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXfludNhrNev"},"source":["dir = '/content/drive/MyDrive/CIMAT/Segundo Semestre/Lenguaje Natural/T04/wikitext-2/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kqpaVO9lrxH2"},"source":["tr_txt, tr_y = get_texts_from_file(dir + 'wiki.train.tokens', dir + 'wiki.test.tokens')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wn3dTceoZA9G"},"source":["### Funciones para Obtener tokens"]},{"cell_type":"code","metadata":{"id":"zCPRr7LyN7XI"},"source":["# Obtengo oraciones\n","def split_by_punctuation (tr_txt) :\n","  sentences = []\n","  for doc in tr_txt :\n","    sentences += doc. split('.')\n","  return sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ToQbC1yZG8FQ"},"source":["# Tokenizo oraciones\n","def get_tokens_in_txt(my_txt) :\n","  # Expresión para quitar caracteres\n","  reg_exp = r\"<?/?[A-Z|a-z]+>?\"\n","\n","  tokens_txt = []\n","  for doc in my_txt :\n","    # coniverto texto a minúsculas y limpio\n","    tmp = re.findall(reg_exp, doc.lower())\n","    tmp = ' '.join(tmp)\n","    # tokenizo\n","    tmp = tknzr.tokenize(tmp)\n","    # Ingoro saltos de línea\n","    if len(tmp) > 0:\n","      # Agreso caracteres de inicio y fin\n","      tokens_txt += [tmp]\n","  return tokens_txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__QDpWBlOH87"},"source":["tr_txt = split_by_punctuation(tr_txt)\n","tr_y   = split_by_punctuation(tr_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Av7J3wc9Ic4o"},"source":["big_split = big.split('\\n')\n","big_txt   = get_tokens_in_txt(big_split)\n","wiki_txt  = get_tokens_in_txt(tr_txt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"185kr0yMZdD3","executionInfo":{"status":"ok","timestamp":1616203932330,"user_tz":360,"elapsed":448,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"d6c97fcc-eea2-4be7-82ad-34df75536c8c"},"source":["wiki_txt[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['valkyria', 'chronicles', 'iii']"]},"metadata":{"tags":[]},"execution_count":272}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTe0HCJDK8fy","executionInfo":{"status":"ok","timestamp":1616203932332,"user_tz":360,"elapsed":407,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"06b795b0-48ac-40bf-e792-ae58f0f2d087"},"source":["print('sentences in big.txt  :', len(big_txt))\n","print('sentences in wiki.txt :', len(wiki_txt))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sentences in big.txt  : 103417\n","sentences in wiki.txt : 90157\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAbBgNQldNhL","executionInfo":{"status":"ok","timestamp":1616203932333,"user_tz":360,"elapsed":381,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"6ad31135-16f5-47f2-d87b-9a151b1537e7"},"source":["# Concateno txt\n","clean_txt = big_txt + wiki_txt\n","len(clean_txt)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["193574"]},"metadata":{"tags":[]},"execution_count":274}]},{"cell_type":"markdown","metadata":{"id":"_sQhzR_MdBSu"},"source":["### Creo corpus"]},{"cell_type":"code","metadata":{"id":"Hh4SLkF4KQ-N"},"source":["# Creo corpus\n","corpus_palabras = []\n","\n","for doc in big_txt :\n","  corpus_palabras += doc\n","\n","for doc in wiki_txt :\n","  corpus_palabras += doc\n","\n","fdist = nltk.FreqDist(corpus_palabras)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CrZnyxjQ--U","executionInfo":{"status":"ok","timestamp":1616203938274,"user_tz":360,"elapsed":2254,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"dbc7ae8a-090b-4502-f7b9-578a52677640"},"source":["print('Total words :', len(corpus_palabras))\n","print('Vocabulary  :', len(fdist))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total words : 2800330\n","Vocabulary  : 41811\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kLjIfHdXEJYO"},"source":["# Creo diccionario de frecuencias\n","fdist = nltk.FreqDist(corpus_palabras)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yep1i517F87M"},"source":["# ordeno diccionario\n","def sortfreqDict(freqdict):\n","  aux = [(freqdict[key], key) for key in freqdict]\n","  aux.sort(key = lambda elem: (elem[0], elem[1]), reverse = True)\n","  return aux"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsvgpkgOGAPA"},"source":["# Recorto vocabulario a 5000 más frecuentes contanto tokens especiales\n","V = sortfreqDict(fdist)\n","# 5000 (incluyendo <unk>)\n","V = V[:5000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QZ0uCXJ_Rw3Q"},"source":["# Creo diccionario de vocabulario\n","dict_indices = dict()\n","count = 0\n","\n","for wigth, word in V:\n","  dict_indices[word] = count\n","  count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4OVMk61fbzP"},"source":["# Enmascaro oraciones con <unk> si no está en mi diccionario\n","def mask_unk(tokens_by_sentence, dict_indices) :\n","  final = []\n","  for doc in tokens_by_sentence :\n","    doc_ = []\n","    for word in doc :\n","      if word not in dict_indices :\n","        doc_.append('<unk>')\n","      else :\n","        doc_.append(word)\n","    final += [doc_]\n","\n","  return final"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSyyPckjgd3r"},"source":["final_tokens = mask_unk(clean_txt, dict_indices)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iHp8gQBSknmP","executionInfo":{"status":"ok","timestamp":1616203944521,"user_tz":360,"elapsed":948,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"0dcd04a1-c13f-41c9-8834-3582b869426b"},"source":["final_tokens[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['the',\n","  'project',\n","  'gutenberg',\n","  'ebook',\n","  'of',\n","  'the',\n","  'adventures',\n","  'of',\n","  'sherlock',\n","  'holmes'],\n"," ['by', 'sir', 'arthur', '<unk>', '<unk>'],\n"," ['in', 'our', 'series', 'by', 'sir', 'arthur', '<unk>', '<unk>'],\n"," ['copyright',\n","  'laws',\n","  'are',\n","  'changing',\n","  'all',\n","  'over',\n","  'the',\n","  'world',\n","  'be',\n","  'sure',\n","  'to',\n","  'check',\n","  'the'],\n"," ['copyright',\n","  'laws',\n","  'for',\n","  'your',\n","  'country',\n","  'before',\n","  '<unk>',\n","  'or',\n","  '<unk>'],\n"," ['this', 'or', 'any', 'other', 'project', 'gutenberg', 'ebook'],\n"," ['this',\n","  '<unk>',\n","  'should',\n","  'be',\n","  'the',\n","  'first',\n","  'thing',\n","  'seen',\n","  'when',\n","  '<unk>',\n","  'this',\n","  'project'],\n"," ['gutenberg',\n","  '<unk>',\n","  'please',\n","  'do',\n","  'not',\n","  'remove',\n","  'it',\n","  'do',\n","  'not',\n","  'change',\n","  'or',\n","  '<unk>',\n","  'the'],\n"," ['<unk>', 'without', 'written', 'permission'],\n"," ['please',\n","  'read',\n","  'the',\n","  'legal',\n","  'small',\n","  'print',\n","  'and',\n","  'other',\n","  'information',\n","  'about',\n","  'the']]"]},"metadata":{"tags":[]},"execution_count":283}]},{"cell_type":"markdown","metadata":{"id":"5Pb-0P1BwOD_"},"source":["### Actualización de diccionario"]},{"cell_type":"code","metadata":{"id":"dridn3iNwQxs"},"source":["# Creo diccionario de frecuencias actualizadas\n","fdist = nltk.FreqDist(np.concatenate(final_tokens).ravel())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUHFqr32clRs"},"source":["fdist = nltk.FreqDist(np.concatenate(final_tokens).ravel())\n","# Recorto vocabulario a 5000 más frecuentes contanto tokens especiales\n","V = sortfreqDict(fdist)\n","# 5000 (incluyendo <unk>)\n","V = V[:5000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wuE89sAewpN8"},"source":["# Creo diccionario de vocabulario\n","dict_indices = dict()\n","count = 0\n","\n","for wigth, word in V:\n","  dict_indices[word] = count\n","  count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VAgRld1baNBr"},"source":["2. Entrene tres modelos de lenguaje: $ P_{unigramas}(w_1^n)$, $ P_{bigramas}(w_1^n) $, $ P_{trigramas}(w_1^n)$. Para cada uno proporcione una interfaz (función) sencilla para $ P_{n−grama}(w_1^n) $  y $ P_{n−grama}(w_1^n | w_{n-N+1}^{n-1})$. Los modelos deben tener una estrategia común para lidiar con secuencias no vistas.\n","Puede optar por un suavizamiento Laplace o un Good-Turing discounting."]},{"cell_type":"markdown","metadata":{"id":"1mRPABPjk1u6"},"source":["### Unigramas"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-lif8JnxMrR","executionInfo":{"status":"ok","timestamp":1616134791413,"user_tz":360,"elapsed":363,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"8b1045b7-9a1b-4a05-d0f7-9233538ac2c9"},"source":["len(V)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5000"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"LL-qaQTmZrxQ"},"source":["def unigram_probabilities(tokens_by_sentence):\n","    # Concateno oraciones con filtrado <unk>\n","    tokens = np.concatenate(tokens_by_sentence).ravel()\n","\n","    # Total de palabras\n","    N = tokens.shape[0]\n","\n","    # Creo diccionario de frecuencias actualizadas\n","    fdist_unigramas = nltk.FreqDist(tokens)\n","    \n","    n = len(fdist_unigramas)\n","\n","    # Creo diccionario de probabilidades con suavisado de Laplace\n","    prob_ngramas = {word: (count + 1.0) / (N + n) for word, count in fdist_unigramas.items()}\n","\n","    return prob_ngramas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxs9r1OjmrmO","executionInfo":{"status":"ok","timestamp":1616193050633,"user_tz":360,"elapsed":4425,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"66c2647e-418c-419f-d19a-862964dd3cca"},"source":["probabilites_unigrams =unigram_probabilities(final_tokens)\n","\n","# Verifico suma\n","sum(probabilites_unigrams.values())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9999999999999991"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"p3RBJxGrav-z"},"source":["def prob_unigrams(test_sentence, probabilites_unigrams, dict_indices) :\n","  probability = 1.0\n","  \n","  test_sentence = test_sentence.split()\n","  for word in  test_sentence :\n","    if word not in dict_indices :\n","      word = '<unk>'\n","      probability *= probabilites_unigrams['<unk>']\n","    else :\n","      probability *= probabilites_unigrams[word]\n","\n","  return probability"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sD4NyEDVvIrR"},"source":["def log_prob_unigrams(test_sentence, probabilites_unigrams, dict_indices) :\n","  probability = 0.0\n","  \n","  test_sentence = test_sentence.split()\n","  for word in  test_sentence :\n","    if word not in dict_indices :\n","      word = '<unk>'\n","      probability += np.log(probabilites_unigrams['<unk>'])\n","    else :\n","      probability += np.log(probabilites_unigrams[word])\n","\n","  return probability"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CuGtZk-0EO5","executionInfo":{"status":"ok","timestamp":1616193057858,"user_tz":360,"elapsed":306,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"38fc0e0d-f15f-4170-d455-aadd382af39e"},"source":["np.log(probabilites_unigrams['<unk>'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-1.9100802867464999"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"cSAAtMbjzS-M"},"source":["**Prueba**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fqxzG72dzIMN","executionInfo":{"status":"ok","timestamp":1616193060500,"user_tz":360,"elapsed":314,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"b7559675-5cf6-430f-f6ed-0800fc90e3f4"},"source":["log_prob_unigrams('hello world', probabilites_unigrams, dict_indices)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-9.247776757807793"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"X0CSlayx64NS"},"source":["### Funciones"]},{"cell_type":"code","metadata":{"id":"BgeTUIACk9Wy"},"source":["def n_grams_probabilities(tokens_by_sentence, V, ngram = 2) :\n","  n      = len(V)\n","  n_gram = ngram - 1 \n","  # Creo padding para crear n-gramas con caracteres especiales\n","  padding = [['<s>'] * n_gram + doc + ['</s>'] * n_gram for doc in tokens_by_sentence]\n","  # Concateno \n","  tokens = np.concatenate(padding).ravel()\n","  # Creo diccionario con caracteres <s> y </s>\n","  n_grams      = nltk.ngrams(tokens, n = ngram)\n","  freq_n_grams = nltk.FreqDist(n_grams)\n","\n","  probabilities = dict()\n","  # Calculo secuencias solo si aparecen\n","  if ngram == 2 :\n","    freq_tokens  = nltk.FreqDist(tokens)\n","\n","    for (first, second), count in freq_n_grams.items() :\n","      count_first = freq_tokens[first]\n","\n","      probabilities[(second, first)] = (count + 1.0) / (count_first + n)\n","  if ngram == 3 :\n","    bigrams      = nltk.ngrams(tokens, n = ngram-1)\n","    freq_bigrams = nltk.FreqDist(bigrams)\n","    \n","    for (first, second, third), count in freq_n_grams.items() :\n","      count_fs = freq_bigrams[(first, second)]\n","      # Suavizado con laplace\n","      probabilities[(third, first, second)] = (count + 1.0) / (count_fs + n)\n","\n","  return probabilities"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rSQRn7lxZRCR"},"source":["def prob_n_grams(test_sentence, probabilites_n_grams, tokens_by_sentence, V, ngram = 2) :\n","  n_gram = ngram - 1 \n","  # enmascaro oracion\n","  test_sentence = mask_unk([test_sentence.split()], dict_indices)\n","  # Concateno inicio y fin de oración\n","  test_sentence = ['<s>'] * n_gram + test_sentence[0] + ['</s>'] * n_gram\n","\n","  # Diccionario de frecuencias unigramas\n","  padding = [['<s>'] * n_gram + doc + ['</s>'] * n_gram for doc in tokens_by_sentence]\n","  # Concateno oraciones con filtrado <unk>\n","  tokens = np.concatenate(padding).ravel()\n","  # Total de palabras\n","  N = tokens.shape[0]\n","  # Creo diccionario de frecuencias actualizadas\n","\n","  n = len(V)\n","\n","  probability = 1.0\n","  if ngram == 2 :\n","    fdist_unigramas = nltk.FreqDist(tokens)\n","\n","    for k in range(len(test_sentence) - 1) :\n","      first  = test_sentence[k]\n","      second = test_sentence[k+1]\n","\n","      if (second, first) in probabilites_n_grams :\n","        probability *= probabilites_n_grams[(second, first)]\n","      # Calculo probabilidad solo si es necesario\n","      else :\n","        freq_first   = fdist_unigramas[first]\n","        freq_first   = 1.0 / (freq_first + n)\n","        probability *= freq_first\n","  if ngram == 3 :\n","    bigrams       = nltk.ngrams(tokens, n = ngram - 1)\n","    fdist_bigrams = nltk.FreqDist(bigrams)\n","\n","    for k in range(len(test_sentence) - 2) :\n","      first  = test_sentence[k]\n","      second = test_sentence[k+1]\n","      third  = test_sentence[k+2]\n","\n","      if (third, first, second) in probabilites_n_grams :\n","        probability *= probabilites_n_grams[(third, first, second)]\n","      # Calculo probabilidad solo si es necesario\n","      else :\n","        count_fs = 0.0\n","        if (first,second) in fdist_bigrams :\n","          count_fs = fdist_bigrams[(first, second)]\n","      \n","        conditional_prop = 1.0 /(count_fs + n)\n","        probability *= conditional_prop\n","\n","  return probability"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"10HuJLsL-SOn"},"source":["def log_prob_n_grams(test_sentence, probabilites_n_grams, tokens_by_sentence, V, ngram = 2) :\n","  n_gram = ngram - 1 \n","  # enmascaro oracion\n","  test_sentence = mask_unk([test_sentence.split()], dict_indices)\n","  # Concateno inicio y fin de oración\n","  test_sentence = ['<s>'] * n_gram + test_sentence[0] + ['</s>'] * n_gram\n","\n","  # Diccionario de frecuencias unigramas\n","  padding = [['<s>'] * n_gram + doc + ['</s>'] * n_gram for doc in tokens_by_sentence]\n","  # Concateno oraciones con filtrado <unk>\n","  tokens = np.concatenate(padding).ravel()\n","  # Total de palabras\n","  N = tokens.shape[0]\n","  # Creo diccionario de frecuencias actualizadas\n","\n","  n = len(V)\n","\n","  probability = 0.0\n","  if ngram == 2 :\n","    fdist_unigramas = nltk.FreqDist(tokens)\n","\n","    for k in range(len(test_sentence) - 1) :\n","      first  = test_sentence[k]\n","      second = test_sentence[k+1]\n","\n","      if (second, first) in probabilites_n_grams :\n","        probability += np.log(probabilites_n_grams[(second, first)])\n","      # Calculo probabilidad solo si es necesario\n","      else :\n","        freq_first   = fdist_unigramas[first]\n","        freq_first   = 1.0 / (freq_first + n)\n","        probability += np.log(freq_first)\n","  if ngram == 3 :\n","    bigrams       = nltk.ngrams(tokens, n = ngram - 1)\n","    fdist_bigrams = nltk.FreqDist(bigrams)\n","\n","    for k in range(len(test_sentence) - 2) :\n","      first  = test_sentence[k]\n","      second = test_sentence[k+1]\n","      third  = test_sentence[k+2]\n","\n","      if (third, second, first) in probabilites_n_grams :\n","        probability += np.log(probabilites_n_grams[(third, first, second)])\n","      # Calculo probabilidad solo si es necesario\n","      else :\n","        count_fs = 0.0\n","        if (first,second) in fdist_bigrams :\n","          count_fs = fdist_bigrams[(first, second)]\n","      \n","        conditional_prop = 1.0 /(count_fs + n)\n","        probability += np.log(conditional_prop)\n","\n","  return probability"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_SitWTf9k4QT"},"source":["### Bigramas"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B7EMrk3f8mio","executionInfo":{"status":"ok","timestamp":1616193077884,"user_tz":360,"elapsed":11391,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"1776926f-cf09-4fc3-e816-2a7bfd14a2d2"},"source":["probabilities_bigrams = n_grams_probabilities(final_tokens, V, ngram = 2)\n","# 320.7323671203257\n","sum(probabilities_bigrams.values())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["320.7323671203257"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrcrO9Qk84Pe","executionInfo":{"status":"ok","timestamp":1616193077885,"user_tz":360,"elapsed":10998,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"c1034d79-a664-48ee-db21-1add44b30b1a"},"source":["# Compruebo probabilidades condicionales fijando una palabra\n","conditional_prop = 0.0\n","\n","for (w2, w1), count in probabilities_bigrams.items() :\n","  if w1 == '<unk>' :\n","    conditional_prop += count\n","\n","conditional_prop"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.999845377991327"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVlRYOZSaQDT","executionInfo":{"status":"ok","timestamp":1616193082666,"user_tz":360,"elapsed":13302,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"fc6f42fa-a6ba-4e58-f612-e57ac59c4685"},"source":["p = prob_n_grams('the world', probabilities_bigrams, final_tokens, V, ngram = 2)\n","np.log(p)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-11.681909044634107"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMPC7oc8lg2X","executionInfo":{"status":"ok","timestamp":1616193088221,"user_tz":360,"elapsed":5538,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"a122808e-494f-408e-b7fd-a2d879ebb9eb"},"source":["log_prob_n_grams('the world', probabilities_bigrams, final_tokens, V, ngram = 2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-11.681909044634107"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"B-gD5Bczk695"},"source":["### Trigramas"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bT9g9DFZ1vrb","executionInfo":{"status":"ok","timestamp":1616193104541,"user_tz":360,"elapsed":21835,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"43fb1d9c-48a0-4d6f-a831-527fd259fe23"},"source":["probabilities_trigrams = n_grams_probabilities(final_tokens, V, ngram = 3)\n","\n","sum(probabilities_trigrams.values())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["759.6022069103449"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DP8aP-etmz7R","executionInfo":{"status":"ok","timestamp":1616193104673,"user_tz":360,"elapsed":21946,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"2dbf5e0f-056d-41e8-81f3-8c240632a7b8"},"source":["# Compruebo probabilidades condicionales fijando una palabra\n","conditional_prop = 0.0\n","for (w3, w2, w1), count in probabilities_trigrams.items() :\n","  if w1 == '<unk>'  and w2 == 'the':\n","    conditional_prop += count\n","conditional_prop"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9687843921961431"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zH3yKSpS-U5w","executionInfo":{"status":"ok","timestamp":1616193113219,"user_tz":360,"elapsed":30473,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"c609d9fe-67e5-46f2-decd-19d6d7d863c2"},"source":["log_prob_n_grams('the world <unk>', probabilities_trigrams, final_tokens, V, ngram = 3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-34.33943242363034"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"lGQotSjGbMjh"},"source":["3. Construya un modelo interpolado con valores $\\lambda $ fijos :\n","\n","$$ \\hat{P} (w_n | w_{n-2} w_{n-1}) = \\lambda_1 P(w_n | w_{n-2} w_{n-1} ) + \\lambda_2 P(w_n | w_{n-2} w_{n-1} ) + \\lambda_3 P(w_n) .$$\n","\n","Para ello experimente con el modelo en particiones de 80%, 10% y 10% para entrenar (train), ajuste de parámetros (val) y prueba (test) respectivamente. Muestre como bajan o suben las perplejidades en validación, finalmente pruebe una vez en test. Explore algunos\n","valores $ \\hat{\\lambda} $ y elija el mejor, i.e., [1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[5, 4, 1] y [.1, .4, .5]."]},{"cell_type":"code","metadata":{"id":"CrvNWgT9skRh"},"source":["def get_n_gram_probability (test_sentence, probabilites_n_grams, fdist_n_grams, N, ngram) :\n","    probability = 1.0\n","    if ngram == 2 :\n","      for k in range(len(test_sentence) - 1) :\n","        first  = test_sentence[k]\n","        second = test_sentence[k+1]\n","\n","        if (second, first) in probabilites_n_grams :\n","          probability *= probabilites_n_grams[(second, first)]\n","        # Calculo probabilidad solo si es necesario\n","        else :\n","          freq_first   = fdist_n_grams[first]\n","          freq_first   = 1.0 / (freq_first + N)\n","          probability *= freq_first\n","    if ngram == 3 :\n","      for k in range(len(test_sentence) - 2) :\n","        first  = test_sentence[k]\n","        second = test_sentence[k+1]\n","        third  = test_sentence[k+2]\n","\n","        if (third, first, second) in probabilites_n_grams :\n","          probability *= probabilites_n_grams[(third, first, second)]\n","        # Calculo probabilidad solo si es necesario\n","        else :\n","          count_fs = 0.0\n","          if (first,second) in fdist_n_grams :\n","            count_fs = fdist_n_grams[(first, second)]\n","        \n","          conditional_prop = 1.0 /(count_fs + N)\n","          probability *= conditional_prop\n","\n","    return probability"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nrbc9a1Fbd9T"},"source":["def compute_perplexity (train_set, test_set, l) :\n","  # actualizo diccionario\n","  fdist = nltk.FreqDist(np.concatenate(train_set).ravel())\n","  V = sortfreqDict(fdist)\n","  V = V[:5000]\n","  # Creo diccionario de vocabulario\n","  dict_indices = dict()\n","  count = 0\n","\n","  for wigth, word in V:\n","    dict_indices[word] = count\n","    count += 1\n","\n","  probabilites_unigrams = unigram_probabilities(train_set)\n","  probabilities_bigrams = n_grams_probabilities(train_set, V, ngram = 2)\n","  probabilities_trigrams = n_grams_probabilities(train_set, V, ngram = 3)\n","\n","  n_gram = 2\n","  # Enmascaro palabras desconocidas\n","  test_set = mask_unk(test_set, dict_indices)\n","\n","  # Agrego padding solo del lado derecho\n","  padding  = [ doc + ['</s>'] * n_gram for doc in test_set]\n","  tokens   = np.concatenate(padding).ravel()\n","  trigrams = nltk.ngrams(tokens, n = 3)\n","\n","  perplexity = 0.0\n","\n","  NP = len(tokens)\n","\n","  # Construyo modelos\n","  n_gram = 2\n","  padding    = [['<s>'] * n_gram + doc + ['</s>'] * n_gram for doc in train_set]\n","  # Concateno oraciones con filtrado <unk>\n","  tokens     = np.concatenate(padding).ravel()\n","  # Total de palabras\n","  N_trigrams = tokens.shape[0]\n","  # Frecuencias bigramas\n","  bigrams       = nltk.ngrams(tokens, n = 2)\n","  fdist_bigrams = nltk.FreqDist(bigrams)\n","\n","  n_gram = 1\n","  padding    = [['<s>'] * n_gram + doc + ['</s>'] * n_gram for doc in train_set]\n","  # Concateno oraciones con filtrado <unk>\n","  tokens     = np.concatenate(padding).ravel()\n","  # Total de palabras\n","  N_bigrams = tokens.shape[0]\n","  # Frecuencias unigramas\n","  fdist_unigramas = nltk.FreqDist(tokens)\n","\n","  perplexity = 0.0\n","\n","  for (first, second, third) in trigrams :\n","    tmp = 0.0\n","    tmp += l[0] * get_n_gram_probability ((first, second, third), probabilities_trigrams, fdist_bigrams, N = N_trigrams, ngram = 3)\n","    tmp += l[1] * get_n_gram_probability ((first, second), probabilities_bigrams, fdist_unigramas, N = N_bigrams, ngram = 2)\n","    tmp += l[2] * prob_unigrams(first, probabilites_unigrams, dict_indices)\n","\n","    perplexity += np.log(tmp)\n","\n","    \n","  \n","  perplexity = - 1.0 / NP * perplexity\n","  return perplexity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wYY_wD88Nnb"},"source":["# Creo clase para guardar probabilidades\n","class language_model :\n","  def __init__(self, train_set, test_set, val_set = None, vocabulary_size = 5000):\n","    fdist = nltk.FreqDist(np.concatenate(train_set).ravel())\n","    V = sortfreqDict(fdist)\n","    V = V[:vocabulary_size]\n","    # Creo diccionario de vocabulario\n","    dict_indices = dict()\n","    count = 0\n","    for wigth, word in V:\n","      dict_indices[word] = count\n","      count += 1\n","\n","    self.train_set = mask_unk(train_set, dict_indices)\n","    self.test_set = mask_unk(test_set, dict_indices)\n","    \n","    # Actualizo diccionario\n","    fdist = nltk.FreqDist(np.concatenate(self.train_set).ravel())\n","    V = sortfreqDict(fdist)\n","    self.V = V\n","    # Creo diccionario de vocabulario\n","    self.dict_indices = dict()\n","    count = 0\n","    for wigth, word in self.V:\n","      self.dict_indices[word] = count\n","      count += 1\n","\n","    self.probabilites_unigrams  = unigram_probabilities(self.train_set)\n","    self.probabilities_bigrams  = n_grams_probabilities(self.train_set, V, ngram = 2)\n","    self.probabilities_trigrams = n_grams_probabilities(self.train_set, V, ngram = 3)\n","\n","    # Preparo modelo para evaluación\n","    n_gram = 2\n","    padding    = [['<s>'] * n_gram + doc + ['</s>'] * n_gram for doc in self.train_set]\n","    # Concateno oraciones con filtrado <unk>\n","    tokens     = np.concatenate(padding).ravel()\n","    # Total de palabras\n","    self.N_trigrams = tokens.shape[0]\n","    # Frecuencias bigramas\n","    bigrams       = nltk.ngrams(tokens, n = 2)\n","    self.fdist_bigrams = nltk.FreqDist(bigrams)\n","\n","    n_gram = 1\n","    padding    = [['<s>'] * n_gram + doc + ['</s>'] * n_gram for doc in self.train_set]\n","    # Concateno oraciones con filtrado <unk>\n","    tokens     = np.concatenate(padding).ravel()\n","    # Total de palabras\n","    self.N_bigrams = tokens.shape[0]\n","    # Frecuencias unigramas\n","    self.fdist_unigramas = nltk.FreqDist(tokens)\n","  \n","  def compute_perplexity(self, l) :\n","    n_gram = 2\n","    # Enmascaro palabras desconocidas\n","    test_set = mask_unk(self.test_set, self.dict_indices)\n","\n","    # Agrego padding solo del lado derecho\n","    padding  = [ doc + ['</s>'] * n_gram for doc in test_set]\n","    tokens   = np.concatenate(padding).ravel()\n","    trigrams = nltk.ngrams(tokens, n = 3)\n","\n","    NP = len(tokens)\n","    perplexity = 0.0\n","\n","    for (first, second, third) in trigrams :\n","      tmp = 0.0\n","      tmp += l[0] * get_n_gram_probability ((first, second, third), self.probabilities_trigrams, self.fdist_bigrams, N = self.N_trigrams, ngram = 3)\n","      tmp += l[1] * get_n_gram_probability ((first, second), self.probabilities_bigrams, self.fdist_unigramas, N = self.N_bigrams, ngram = 2)\n","      tmp += l[2] * prob_unigrams(first, self.probabilites_unigrams, self.dict_indices)\n","\n","      perplexity += np.log(tmp)\n","\n","    perplexity = - 1.0 / NP * perplexity\n","\n","    return perplexity\n","\n","  def sentence_probability(self, sentence, l) :\n","    n_gram = 2\n","    # Enmascaro palabras desconocidas\n","    sentence = mask_unk(sentence, self.dict_indices)\n","    \n","    '''# Si no está en diccionario, ignoro oración\n","    for word in sentence[0] :\n","      if word == '<unk>' :\n","        return 0.0'''\n","    \n","    padding  = [['</s>'] + sentence[0] + ['</s>']]\n","\n","    tokens   = np.concatenate(padding).ravel()\n","    trigrams = nltk.ngrams(tokens, n = 3)\n","\n","    probability = 1.0\n","\n","    for (first, second, third) in trigrams :\n","      tmp = 0.0\n","      # Compruebo valores de lambda\n","      if l[0] == 0.0 :\n","        tmp += 0.0\n","      else :\n","        tmp += l[0] * get_n_gram_probability ((first, second, third), self.probabilities_trigrams, self.fdist_bigrams, N = self.N_trigrams, ngram = 3)\n","\n","      if l[1] == 0.0 :\n","        tmp += 0.0\n","\n","      else :\n","        tmp *= l[1] * get_n_gram_probability ((first, second), self.probabilities_bigrams, self.fdist_unigramas, N = self.N_bigrams, ngram = 2)\n","\n","      if l[2] == 0.0 :\n","        tmp += 0.0\n","      else :\n","        tmp += l[2] * prob_unigrams(first, self.probabilites_unigrams, self.dict_indices)\n","\n","      probability *= tmp\n","\n","    return probability"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vRVR1l6b5GW"},"source":["from argparse import Namespace\n","args      = Namespace()\n","args.seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8C7n38NFHYM"},"source":["from sklearn.model_selection import train_test_split\n","l = [[1/3, 1/3, 1/3], [0.4, 0.4, 0.2], [0.2, 0.4, 0.4], [0.5, 0.4, 0.1], [0.1, 0.4, 0.5]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPucT2hxAv5R","executionInfo":{"status":"ok","timestamp":1616126005047,"user_tz":360,"elapsed":108555,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"d316f006-2bae-44cd-9864-593ef8f31145"},"source":["X_train, X_val = train_test_split(final_tokens, train_size = 0.9, test_size = 0.1, random_state = args.seed)\n","\n","mymodel = language_model(X_train, X_val)\n","\n","for lam in l :\n","  pp = mymodel.compute_perplexity(lam)\n","  print('Perplexity : ', np.exp(pp), 'with ', lam)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Perplexity :  136.7051990998809 with  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n","Perplexity :  158.26613453034238 with  [0.4, 0.4, 0.2]\n","Perplexity :  121.21907121268646 with  [0.2, 0.4, 0.4]\n","Perplexity :  206.30989680085594 with  [0.5, 0.4, 0.1]\n","Perplexity :  112.08267053566311 with  [0.1, 0.4, 0.5]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DCFLT4oBnjoj"},"source":["X_train, X_val = train_test_split(final_tokens, train_size = 0.8, test_size = 0.1, random_state = args.seed)\n","mymodel = language_model(X_train, X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJsRInOvENnp","executionInfo":{"status":"ok","timestamp":1616134958513,"user_tz":360,"elapsed":94378,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"06cf75d2-aab5-4cbf-c6cb-9d2d9407f38d"},"source":["# Pruebo diferentes lambdas\n","for lam in l :\n","  pp = mymodel.compute_perplexity(lam)\n","  print('Perplexity : ', np.exp(pp), 'with ', lam)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Perplexity :  142.329470435333 with  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n","Perplexity :  165.4256088629811 with  [0.4, 0.4, 0.2]\n","Perplexity :  125.99737146444386 with  [0.2, 0.4, 0.4]\n","Perplexity :  216.64448277849914 with  [0.5, 0.4, 0.1]\n","Perplexity :  116.23318676055801 with  [0.1, 0.4, 0.5]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M7bdgkEjcX5r"},"source":["## 3.2 Corrección ortográfica y gramatical"]},{"cell_type":"markdown","metadata":{"id":"WzuheIhzciGC"},"source":["Implemente una función spell_correction que combine el uso del generador de candidatos de Norvig con el modelo de lenguaje entrenado en el punto anterior. La idea es detectar el error ortográco, generar candidatos, pero diseñar alguna estrategia para dejar que el modelo de lenguaje elija el mejor candidato para aplicar la corrección. Sí alguna palabra de entrada no está en su vocabulario reemplácela por \\<unk>."]},{"cell_type":"markdown","metadata":{"id":"Rba8ReFe4JOL"},"source":["### Norvig"]},{"cell_type":"code","metadata":{"id":"rPRwg0lV4IY5"},"source":["import re\n","from collections import Counter\n","\n","def words(text): return re.findall(r'\\w+', text.lower())\n","\n","WORDS = Counter(words(big))\n","\n","def P(word, N=sum(WORDS.values())): \n","    \"Probability of `word`.\"\n","    return WORDS[word] / N\n","\n","def correction(word): \n","    \"Most probable spelling correction for word.\"\n","    return max(candidates(word), key=P)\n","\n","def candidates(word): \n","    \"Generate possible spelling corrections for word.\"\n","    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n","\n","def known(words): \n","    \"The subset of `words` that appear in the dictionary of WORDS.\"\n","    return set(w for w in words if w in WORDS)\n","\n","def edits1(word):\n","    \"All edits that are one edit away from `word`.\"\n","    letters    = 'abcdefghijklmnopqrstuvwxyz'\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","    deletes    = [L + R[1:]               for L, R in splits if R]\n","    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","    inserts    = [L + c + R               for L, R in splits for c in letters]\n","    return set(deletes + transposes + replaces + inserts)\n","\n","def edits2(word): \n","    \"All edits that are two edits away from `word`.\"\n","    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5J1uvBDchVy"},"source":["import itertools\n","def spell_correction(input_text) :\n","  '''\n","  input_text: a text with possible misspellings represented as a word list\n","  Returns: a list with corrected misspellings\n","  '''\n","  # Busco Candidatos\n","  can = []\n","  for word in input_text :\n","    can.append(candidates(word))\n","\n","  # Hago todas las posibles oraciones\n","  tupples = itertools.product(*can)\n","  \n","  # Convierto tuplas a lista\n","  corrected_words = []\n","  for element in tupples:\n","    corrected_words.append(list(element))\n","\n","  # Obtengo probabilidades\n","  p = []\n","  for candi in corrected_words :\n","    p += [mymodel.sentence_probability([candi], [0.1, 0.4, 0.5])]\n","  \n","  # Me quedo con la de mayor probabilidad\n","  max = 0\n","  id = 0\n","  for i in range(len(p)) :\n","    if max < p[i] :\n","      max = p[i]\n","      id = i\n","\n","  return corrected_words[id]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VPZu-Jx6BjJZ"},"source":["def grammar_candidates(input_txt) :\n","  correct = []\n","  for word in input_txt :\n","    correct += [correction(word)]\n","  \n","  return correct"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eladdV8CN3W7","executionInfo":{"status":"ok","timestamp":1616193247268,"user_tz":360,"elapsed":55514,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"49a2a43f-8012-4ae9-ccaf-e37ed8c7501c"},"source":["example = ['my', 'countr' , 'is', 'biig']\n","spell_correction(example)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['my', 'country', 'is', 'big']"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7t0z8ZFJ_2I","executionInfo":{"status":"ok","timestamp":1616193260211,"user_tz":360,"elapsed":297,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"0f890441-6233-422a-fc83-bf8f3fc502ad"},"source":["grammar_candidates(example)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['my', 'count', 'is', 'big']"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"AI4xiDEoQvMU"},"source":["target_txt = [['i', 'hav', 'a', 'ham'],\n","              ['my', 'countr' , 'is', 'biig'],\n","              ['i', 'want', 't0o', 'eat'],\n","              ['the', 'science', '0f', 'computr']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGfbqEP7BRZz","executionInfo":{"status":"ok","timestamp":1616193279418,"user_tz":360,"elapsed":271,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"ab39a8ac-9ffe-4d29-a3be-2042fe238540"},"source":["for t_txt in target_txt :\n","  print('Original: ', t_txt)\n","  print('Norvig  : ',grammar_candidates(t_txt))\n","  print('Modelo  : ',spell_correction(t_txt))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  ['i', 'hav', 'a', 'ham']\n","Norvig  :  ['i', 'had', 'a', 'ham']\n","Modelo  :  ['i', 'ham', 'a', 'ham']\n","Original:  ['my', 'countr', 'is', 'biig']\n","Norvig  :  ['my', 'count', 'is', 'big']\n","Modelo  :  ['my', 'country', 'is', 'big']\n","Original:  ['i', 'want', 't0o', 'eat']\n","Norvig  :  ['i', 'want', 'to', 'eat']\n","Modelo  :  ['i', 'want', 'to', 'eat']\n","Original:  ['the', 'science', '0f', 'computr']\n","Norvig  :  ['the', 'science', 'of', 'computer']\n","Modelo  :  ['the', 'science', 'of', 'computer']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EQPL5yZ1c2a2"},"source":["Explica en que consiste tu estrategia; máximo dos párrafos. Escoge algunos ejemplos que ilustren el comportamiento de su estrategia. Abajo se proporcionan algunas sugerencias de pruebas simples y su salida esperada, aunque puede usar otras si así lo desea."]},{"cell_type":"markdown","metadata":{"id":"MjK90QbBLudT"},"source":["**Nota.** La idea es generar los candidatos con el función de Norvig, luego calcular todas las oraciones posibles con los candidatos y luego calcular las probabilidades de acuerdo al modelo realizado y devolver el más probable."]},{"cell_type":"markdown","metadata":{"id":"X4N8zUBydBNR"},"source":["## 3.3 Correción Gramatical"]},{"cell_type":"markdown","metadata":{"id":"i2bXvvMZdFQw"},"source":["Para el escenario en el que no hay errores ortográficos, proponga una estrategia simple para sugerir corrección gramatical. Discuta en no más de dos párrafos las ventajas/desventajas de su estrategia. Escoja 3 ejemplos para ilustrar dónde su estrategia funciona y 3 en los que no. "]},{"cell_type":"code","metadata":{"id":"IpecB8pBP2vI"},"source":["def edits1(word):\n","    \"All edits that are one edit away from `word`.\"\n","    letters    = 'abcdefghijklmnopqrstuvwxyz'\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","    deletes    = [L + R[1:]               for L, R in splits if R]\n","    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n","    return set(deletes + replaces)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0EaHKA0cP0Uf"},"source":["### Ejemplos Positivos"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hcm8pP5FPqE_","executionInfo":{"status":"ok","timestamp":1616193291789,"user_tz":360,"elapsed":467,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"0cc7195a-92d6-49fd-ae5d-6b59a3fc2019"},"source":["example = ['the', 'scienceses', 'of', 'computerses']\n","\n","print('Original: ', example)\n","print('Norvig  : ',grammar_candidates(example))\n","print('Modelo  : ',spell_correction(example))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  ['the', 'scienceses', 'of', 'computerses']\n","Norvig  :  ['the', 'sciences', 'of', 'computers']\n","Modelo  :  ['the', 'sciences', 'of', 'computers']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpvKM35CQK__","executionInfo":{"status":"ok","timestamp":1616193293348,"user_tz":360,"elapsed":356,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"e7b668ee-5c64-42c6-a363-1e79762af9bb"},"source":["example = ['thi', 'big', 'countri']\n","\n","print('Original: ', example)\n","print('Norvig  : ',grammar_candidates(example))\n","print('Modelo  : ',spell_correction(example))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  ['thi', 'big', 'countri']\n","Norvig  :  ['the', 'big', 'country']\n","Modelo  :  ['the', 'big', 'country']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5LsWUyJRg15","executionInfo":{"status":"ok","timestamp":1616193293491,"user_tz":360,"elapsed":283,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"f553b6e7-d7a4-40a7-976f-ab2c970f5196"},"source":["example = ['i', 'wana', 'go', 'to', 'the', 'batroom']\n","\n","print('Original: ', example)\n","print('Norvig  : ',grammar_candidates(example))\n","print('Modelo  : ',spell_correction(example))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  ['i', 'wana', 'go', 'to', 'the', 'batroom']\n","Norvig  :  ['i', 'want', 'go', 'to', 'the', 'bedroom']\n","Modelo  :  ['i', 'want', 'go', 'to', 'the', 'bedroom']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rMfHVX29P3qu"},"source":["### Ejemplos Negativos"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yVV0DtlAOy1s","executionInfo":{"status":"ok","timestamp":1616193294624,"user_tz":360,"elapsed":274,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"2308a2be-2853-4d5e-f43b-1aaa2e404737"},"source":["example = ['the', 'science', 'off', 'maths']\n","\n","print('Original: ', example)\n","print('Norvig  : ',grammar_candidates(example))\n","print('Modelo  : ',spell_correction(example))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  ['the', 'science', 'off', 'maths']\n","Norvig  :  ['the', 'science', 'off', 'baths']\n","Modelo  :  ['the', 'science', 'off', 'matas']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEpARJoqPzI_","executionInfo":{"status":"ok","timestamp":1616193295522,"user_tz":360,"elapsed":275,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"37512a19-6afe-4f88-8593-0d91478522ec"},"source":["example = ['were', 'iz', 'the', 'bathroom']\n","\n","print('Original: ', example)\n","print('Norvig  : ',grammar_candidates(example))\n","print('Modelo  : ',spell_correction(example))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  ['were', 'iz', 'the', 'bathroom']\n","Norvig  :  ['were', 'in', 'the', 'bathroom']\n","Modelo  :  ['were', 'il', 'the', 'bathroom']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q6nsSPmsSXoW","executionInfo":{"status":"ok","timestamp":1616193296034,"user_tz":360,"elapsed":222,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"2de9de82-b8c4-436e-95f3-c6eb5e292d7c"},"source":["example = ['the', 'bij', 'montain', 'is', 'hery']\n","\n","print('Original: ', example)\n","print('Norvig  : ',grammar_candidates(example))\n","print('Modelo  : ',spell_correction(example))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:  ['the', 'bij', 'montain', 'is', 'hery']\n","Norvig  :  ['the', 'big', 'contain', 'is', 'her']\n","Modelo  :  ['the', 'big', 'contain', 'is', 'very']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"03jb7mZUSppF"},"source":["La estrategia fue eliminar de las correcciones de Norvig aquellas que no corresponden a errores gramaticales como las transposiciones y las inserciones dado que la palabra está ''completa'' pero no corresponde a errores ortográficos."]},{"cell_type":"markdown","metadata":{"id":"Rq-Uy5oldSdV"},"source":["## 3.4 Auto-completado\n"]},{"cell_type":"markdown","metadata":{"id":"VgFer3hvdY7H"},"source":["Use su modelo de lenguaje para diseñar una función que sugiera la siguiente palabra dada una secuencia previa."]},{"cell_type":"code","metadata":{"id":"eHNFDJfedE1t"},"source":["def autocompletate_and_candidates(t_text, model, l, show_other_candidates = 5) :\n","  # Creo todas las posibles oraciones\n","  sentences = []\n","  for word in model.dict_indices :\n","    sentences.append(t_text + [word])\n","\n","  # Obtengo probabilidades\n","  p = []\n","  id = 0\n","  for sentence in sentences :\n","    p += [[model.sentence_probability([sentence], l), id]]\n","    id += 1\n","  # Ordeno oraciones por probabilidad\n","  p.sort(reverse = True)\n","\n","  most_freq = [sentences[p[i+1][1]] for i in range(show_other_candidates)]\n","  \n","  return sentences[p[0][1]], most_freq\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BAuDF7arddQD"},"source":["Escoja 3 ejemplos para ilustrar dónde su estrategia funciona y 3 en los que no. En un párrafo explique brevemente porque falló."]},{"cell_type":"markdown","metadata":{"id":"tHl26GuBZxWk"},"source":["### Ejemplos Positivos"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2thDV5MZTxHE","executionInfo":{"status":"ok","timestamp":1616193303987,"user_tz":360,"elapsed":406,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"c7447513-52af-499f-86da-52dd87cf8096"},"source":["s = ['i', 'would', 'like', 'to', 'visit', 'new']\n","\n","top, top_five = autocompletate_and_candidates(s, mymodel,[0.1, 0.4, 0.5])\n","print('Top option     : ',top)\n","for t in top_five :\n","  print('Top candidates : ',t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top option     :  ['i', 'would', 'like', 'to', 'visit', 'new', 'york']\n","Top candidates :  ['i', 'would', 'like', 'to', 'visit', 'new', 'england']\n","Top candidates :  ['i', 'would', 'like', 'to', 'visit', 'new', 'zealand']\n","Top candidates :  ['i', 'would', 'like', 'to', 'visit', 'new', 'jersey']\n","Top candidates :  ['i', 'would', 'like', 'to', 'visit', 'new', 'orleans']\n","Top candidates :  ['i', 'would', 'like', 'to', 'visit', 'new', 'bone']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUZBB8VKZ02x","executionInfo":{"status":"ok","timestamp":1616193307804,"user_tz":360,"elapsed":473,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"ca82919f-2665-4fca-d11d-62b2e772a06d"},"source":["s = ['i', 'am', 'drinking']\n","\n","top, top_five = autocompletate_and_candidates(s, mymodel,[0.1, 0.4, 0.5])\n","print('Top option     : ',top)\n","for t in top_five :\n","  print('Top candidates : ',t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top option     :  ['i', 'am', 'drinking', 'water']\n","Top candidates :  ['i', 'am', 'drinking', 'he']\n","Top candidates :  ['i', 'am', 'drinking', 'of']\n","Top candidates :  ['i', 'am', 'drinking', 'and']\n","Top candidates :  ['i', 'am', 'drinking', 'from']\n","Top candidates :  ['i', 'am', 'drinking', 'a']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5yM7plH6b9UX","executionInfo":{"status":"ok","timestamp":1616193313249,"user_tz":360,"elapsed":773,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"61833de4-e481-4246-81dd-04e798cfd8f1"},"source":["s = ['i','have', 'to', 'work', 'at', 'the']\n","\n","top, top_five = autocompletate_and_candidates(s, mymodel,[0.1, 0.4, 0.5])\n","print('Top option     : ',top)\n","for t in top_five :\n","  print('Top candidates : ',t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top option     :  ['i', 'have', 'to', 'work', 'at', 'the', 'same']\n","Top candidates :  ['i', 'have', 'to', 'work', 'at', 'the', 'time']\n","Top candidates :  ['i', 'have', 'to', 'work', 'at', 'the', 'first']\n","Top candidates :  ['i', 'have', 'to', 'work', 'at', 'the', 'end']\n","Top candidates :  ['i', 'have', 'to', 'work', 'at', 'the', 'other']\n","Top candidates :  ['i', 'have', 'to', 'work', 'at', 'the', 'french']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uCJ96Hi4aD_G"},"source":["### Ejemplos negativos"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B27epF1sb_Jh","executionInfo":{"status":"ok","timestamp":1616193317769,"user_tz":360,"elapsed":1036,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"a54a0adc-aab5-42a9-96cd-75badf3d47b0"},"source":["s = ['the', 'mountain', 'is', 'full', 'of']\n","\n","top, top_five = autocompletate_and_candidates(s, mymodel,[0.1, 0.4, 0.5])\n","print('Top option     : ',top)\n","for t in top_five :\n","  print('Top candidates : ',t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top option     :  ['the', 'mountain', 'is', 'full', 'of', 'the']\n","Top candidates :  ['the', 'mountain', 'is', 'full', 'of', 'a']\n","Top candidates :  ['the', 'mountain', 'is', 'full', 'of', 'an']\n","Top candidates :  ['the', 'mountain', 'is', 'full', 'of', 'that']\n","Top candidates :  ['the', 'mountain', 'is', 'full', 'of', 'life']\n","Top candidates :  ['the', 'mountain', 'is', 'full', 'of', 'these']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ClbDW4nxdeL-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616193320371,"user_tz":360,"elapsed":416,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"76a3ceed-f47c-4ad2-a16a-03f86ef32e64"},"source":["s = ['the', 'big', 'lake', 'is', 'full', 'of']\n","\n","top, top_five = autocompletate_and_candidates(s, mymodel,[0.1, 0.4, 0.5])\n","print('Top option     : ',top)\n","for t in top_five :\n","  print('Top candidates : ',t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top option     :  ['the', 'big', 'lake', 'is', 'full', 'of', 'the']\n","Top candidates :  ['the', 'big', 'lake', 'is', 'full', 'of', 'a']\n","Top candidates :  ['the', 'big', 'lake', 'is', 'full', 'of', 'an']\n","Top candidates :  ['the', 'big', 'lake', 'is', 'full', 'of', 'that']\n","Top candidates :  ['the', 'big', 'lake', 'is', 'full', 'of', 'life']\n","Top candidates :  ['the', 'big', 'lake', 'is', 'full', 'of', 'these']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RinbZFQEbyGY","executionInfo":{"status":"ok","timestamp":1616193326885,"user_tz":360,"elapsed":661,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"7cd2c34d-1ce1-475b-e94b-8aa0204d47e3"},"source":["s = ['i', 'have', 'a', 'blue']\n","\n","top, top_five = autocompletate_and_candidates(s, mymodel,[0.1, 0.4, 0.5])\n","print('Top option     : ',top)\n","for t in top_five :\n","  print('Top candidates : ',t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Top option     :  ['i', 'have', 'a', 'blue', 'or']\n","Top candidates :  ['i', 'have', 'a', 'blue', 'coat']\n","Top candidates :  ['i', 'have', 'a', 'blue', 'moon']\n","Top candidates :  ['i', 'have', 'a', 'blue', 'eyes']\n","Top candidates :  ['i', 'have', 'a', 'blue', 'and']\n","Top candidates :  ['i', 'have', 'a', 'blue', 'water']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lUNmsyzLdo9P"},"source":["**Nota**. Los ejemplos en los que el modelo falló fueron oraciones cortas, esto podría deberse a que la probabilidades para las oraciones sean muy similares para todas las palabras del diccionario. Además, si no se le da mucho contexto a las oraciones, parece que trata de completar con conectores o preposiciones. Para el último ejemplo que no funcionó se observa que algunas de las opciones del top_five tienen más sentido que la primera, podría tatarse de la frecuencia de las stopwords."]},{"cell_type":"markdown","metadata":{"id":"d67Mzc8ghjJh"},"source":["## 4 Generación de Texto"]},{"cell_type":"markdown","metadata":{"id":"g1pR-vB5hoI5"},"source":["Para esta parte reentrenará su modelo del lenguaje interpolado:\n","\n","$$ \\hat{P} (w_n | w_{n-2} w_{n-1}) = \\lambda_1 P(w_n | w_{n-2} w_{n-1} ) + \\lambda_2 P(w_n | w_{n-2} w_{n-1} ) + \\lambda_3 P(w_n) .$$\n","\n","realice las siguientes actividades"]},{"cell_type":"markdown","metadata":{"id":"CKH4e2IPiFQV"},"source":["1. Para los tuits de agresividad use solo palabras en minúscula, quite signos de puntuación, números, etc. Agregue tokens especiales de \\<s\\> y \\<s\\> a cada ''tuit''.\n"]},{"cell_type":"code","metadata":{"id":"vI9uPz9ViyNX"},"source":["def get_texts_from_file (path_corpus, path_truth) :\n","  tr_txt = []\n","  tr_y   = []\n","\n","  with open (path_corpus, \"r\") as f_corpus, open(path_truth, \"r\") as f_truth:\n","    for twitt in f_corpus:\n","      twitt = twitt.replace('\\n', ' ')\n","      tr_txt += [twitt]\n","    for label in f_truth:\n","      tr_y   += [label]\n","  return tr_txt, tr_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgJJELZ-hfwt"},"source":["def ad_star_end_caracters(tokens) :\n","  final_tokens = []\n","  for doc in tokens :\n","    final_tokens += [['<s>'] + doc + ['</s>']]\n","\n","  return final_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qj5vmBTmgjEZ"},"source":["dir = '/content/drive/MyDrive/CIMAT/Segundo Semestre/Lenguaje Natural/Práctica 3'\n","tr_txt, tr_y = get_texts_from_file(dir + '/mex_train.txt', dir + '/mex_train_labels.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZNGLmhN9hSuE"},"source":["clean_txt = get_tokens_in_txt(tr_txt)\n","final_tokens = ad_star_end_caracters(clean_txt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1DpjYPsFp2lY","executionInfo":{"status":"ok","timestamp":1616203982014,"user_tz":360,"elapsed":254,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"47f066f5-7e97-4ba9-e355-3ec7dcf3c108"},"source":["final_tokens[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>',\n"," 'lo',\n"," 'peor',\n"," 'de',\n"," 'todo',\n"," 'es',\n"," 'que',\n"," 'no',\n"," 'me',\n"," 'dan',\n"," 'por',\n"," 'un',\n"," 'tiempo',\n"," 'y',\n"," 'luego',\n"," 'vuelven',\n"," 'estoy',\n"," 'hasta',\n"," 'la',\n"," 'verga',\n"," 'de',\n"," 'estl',\n"," '</s>']"]},"metadata":{"tags":[]},"execution_count":288}]},{"cell_type":"markdown","metadata":{"id":"vo5sQJgXiFNv"},"source":["2. Proponga una estrategia con base en Expectation Maximization para encontrar los valores de interpolación en $\\hat{P}_{agresivo} $ usando todo el dataset de agresividad. Para ello experimente con el modelo en particiones de 80%, 10% y 10% para entrenar (train), ajuste de parámetros (val) y prueba (test) respectivamente. Muestre como bajan\n","las perplejidades en 5 iteraciones que usted elija (de todas las que sean necesarias de acuerdo a su EM) en validación, y pruebe una vez en test. Si no logró diseñar algo basado en EM (-10pts) explore algunos valores $\\hat{lambda} $ en algunas pruebas y elija el mejor, por ejemplo: $[1/3, 1/3, 1/3],[.4, .4, .2],[.2, .4, .4],[5, 4, 1]$ y $[.1, .4, .5]$."]},{"cell_type":"code","metadata":{"id":"cY_o_3_P-KYb"},"source":["# Separo conjunto \n","X_train, X_val = train_test_split(final_tokens, train_size = 0.8, test_size = 0.2, random_state = args.seed)\n","X_val, X_test  = train_test_split(X_val, train_size = 0.5, test_size = 0.5, random_state = args.seed)\n","\n","mymodel = language_model(X_train, X_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MHwtWGW-uqa_"},"source":["def expectation_maximization (model, iterations = 5) :\n","  ngrams = 3\n","  lmdas = [1.0/ngrams for i in range(ngrams)]\n","  # concateno tokens en conjunto de validación\n","  flatten_val_set = np.concatenate(model.test_set).ravel()\n","  # tokens en val_set\n","  M = flatten_val_set.shape[0]\n","  # Vectores de distribuciones q_m\n","  q = np.zeros((M, ngrams), dtype=np.float)\n","\n","  for iter in range(iterations) :\n","    # Ciclo sobre tokens de validación\n","    for m in range(M) :\n","      sentence = flatten_val_set[:m+1]\n","      q[m][0] = model.sentence_probability(sentence, [lmdas[0], 0.0, 0.0])\n","      q[m][1] = model.sentence_probability(sentence, [0.0, lmdas[1], 0.0])\n","      q[m][2] = model.sentence_probability(sentence, [0.0, 0.0, lmdas[2]])\n","      # Normalizo vector\n","      q[m] = q[m] / np.linalg.norm(q[m])\n","    # Update lambdas\n","    for z in range(ngrams) :\n","      lmdas[z] = 1.0 / M *sum(q[:,z])\n","    # Observo suma de lambdas\n","    print('Suma lambdas: ', sum(lmdas))\n","    # Observo perplejidad\n","    print('Perplejidad : ', np.exp(model.compute_perplexity(lmdas)))\n","  return lmdas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7KYuo9YFfgH","executionInfo":{"status":"ok","timestamp":1616206699678,"user_tz":360,"elapsed":1647468,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"50b2095e-9244-421d-b647-0fe746436ad6"},"source":["l = expectation_maximization(mymodel, iterations = 5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Suma lambdas:  1.000006642496347\n","Perplejidad :  190.01265766450123\n","Suma lambdas:  0.9999999999999999\n","Perplejidad :  190.01526640872692\n","Suma lambdas:  0.9999999999999999\n","Perplejidad :  190.01526640872692\n","Suma lambdas:  0.9999999999999999\n","Perplejidad :  190.01526640872692\n","Suma lambdas:  0.9999999999999999\n","Perplejidad :  190.01526640872692\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IT7KpxhkbTKx","executionInfo":{"status":"ok","timestamp":1616206707920,"user_tz":360,"elapsed":448,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"472118d0-5cb4-43c5-bbae-57d1c33d78df"},"source":["# Valores de lambda\n","l"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.0, 0.0, 0.9999999999999999]"]},"metadata":{"tags":[]},"execution_count":318}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o7AS-IQNbsSS","executionInfo":{"status":"ok","timestamp":1616206908105,"user_tz":360,"elapsed":310,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"7db4a6f4-116c-4bac-f93e-b5fb0eeec1b1"},"source":["mymodel.test_set = X_test\n","np.exp(mymodel.compute_perplexity(l))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["189.7834877912424"]},"metadata":{"tags":[]},"execution_count":321}]},{"cell_type":"markdown","metadata":{"id":"q66_iKAriFKi"},"source":["3. Haga una función \"tuitear\" basado en su modelo de lenguaje $\\hat{P}_{agresivo}$ del último punto. El modelo deberá poder parar automáticamente cuando genere algún n-grama con el símbolo de terminación de tuit al final (e.g., \"\\</s\\>\"), o 50 palabras. Proponga algo para que en los últimos tokens sea más probable generar el token \"\\</s\\>\")."]},{"cell_type":"code","metadata":{"id":"9qFbsQU8fkXY"},"source":["def tuitear (pivot, model, lmda) :\n","  final_sentence = pivot.copy()\n","  for i in range(50) :\n","    top, top_five = autocompletate_and_candidates(pivot, model, [0.1, 0.4, 0.5])\n","    pivot.pop(0)\n","    pivot.append(top[-1])\n","\n","    final_sentence += [top[-1]]\n","    if top[-1] == '</s>' :\n","      break\n","\n","  return final_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fd8GuEMZk-v8","executionInfo":{"status":"ok","timestamp":1616210894188,"user_tz":360,"elapsed":833,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"816bc998-c387-48c9-f457-c44c780a69b9"},"source":["pivot = ['<s>', '<s>', 'hola']\n","\n","# Genero oración de acuerdo a modelo\n","tuitear(pivot, mymodel, [0.1, 0.1, 0.8])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>', '<s>', 'hola', 'pinche', 'putita', '</s>']"]},"metadata":{"tags":[]},"execution_count":391}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5uJYhsphqCSq","executionInfo":{"status":"ok","timestamp":1616210598005,"user_tz":360,"elapsed":740,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"0892b05c-919f-4e3b-9789-da5f632c3801"},"source":["pivot = ['<s>', '<s>', 'me', 'vale']\n","\n","# Genero oración de acuerdo a modelo\n","tuitear(pivot, mymodel, [0.1, 0.1, 0.8])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>', '<s>', 'me', 'vale', 'verga', '</s>']"]},"metadata":{"tags":[]},"execution_count":379}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iynZdgbuqKxK","executionInfo":{"status":"ok","timestamp":1616210642763,"user_tz":360,"elapsed":777,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"93d62a53-fefe-4537-c591-94000da02c15"},"source":["pivot = ['<s>', '<s>', 'los', 'gays', 'son', 'muy']\n","\n","# Genero oración de acuerdo a modelo\n","tuitear(pivot, mymodel, [0.1, 0.1, 0.8])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>', '<s>', 'los', 'gays', 'son', 'muy', 'loca', '</s>']"]},"metadata":{"tags":[]},"execution_count":383}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KPuPR9gqVkh","executionInfo":{"status":"ok","timestamp":1616210660880,"user_tz":360,"elapsed":698,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"5d0398c5-c7bf-4ff5-adea-0930f0be946f"},"source":["pivot = ['<s>', '<s>', 'las', 'mujeres', 'son']\n","\n","# Genero oración de acuerdo a modelo\n","tuitear(pivot, mymodel, [0.1, 0.1, 0.8])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>', '<s>', 'las', 'mujeres', 'son', 'putas', '</s>']"]},"metadata":{"tags":[]},"execution_count":385}]},{"cell_type":"markdown","metadata":{"id":"K4pu-1UcrXH2"},"source":["**Nota**. '<\\<s>' se hace más probable debido a la forma en la que se formó el corpus, añadiendo este token en cada oración."]},{"cell_type":"markdown","metadata":{"id":"ueiP9xg-fkzF"},"source":["4. Use la intuición que ha ganado en esta tarea y los datos de las mañaneras\n","para entrenar un modelo de lenguaje AMLO. Haga una un función \"dar_conferencia()\". Puede entrenar su modelo con un solo string gigante de todas las intervenciones de AMLO, sin tokens \"\\<s>\" y \"\\</s>\". Generé un discurso de 300 palabras y detenga al modelo de forma abrupta."]},{"cell_type":"code","metadata":{"id":"g_bp7yA6zmVa"},"source":["dir = '/content/drive/MyDrive/CIMAT/Segundo Semestre/Lenguaje Natural/Conferencias/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSlVzryKto8-"},"source":["import glob\n","import io\n","# Creo String vacío\n","conferencias_str = \"\"\n","for my_file in glob.glob(dir + \"./estenograficas_limpias_por_fecha/*\"):\n","    conferencias_str += open(my_file, \"r\", encoding = \"utf-8\").read()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oy9pZZmN4h78"},"source":["# Tokenizo oraciones\n","def get_tokens_in_txt(my_txt) :\n","  # Expresión para quitar caracteres\n","  reg_exp = r'\\w+'\n","\n","  tokens_txt = []\n","  for doc in my_txt :\n","    # coniverto texto a minúsculas y limpio\n","    tmp = re.findall(reg_exp, doc.lower())\n","    # Ingoro aquellas oraciones de longitud menor que 5\n","    # Intervenciones tienen al menos cinco tokens ['presidente', 'andrés', 'manuel', 'lópez', 'obrador']\n","    # Ignoro oraciones que empeicen con números\n","    if len(tmp) > 5 and tmp[0].isnumeric() == False :\n","      # Agreso caracteres de inicio y fin\n","      tokens_txt += [tmp]\n","  return tokens_txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8bmSaWT13NuO","executionInfo":{"status":"ok","timestamp":1616214886127,"user_tz":360,"elapsed":2689,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"23f5a156-6685-4319-e93a-e4165718a34e"},"source":["AMLO_tokens = get_tokens_in_txt(conferencias_str.split('\\n'))\n","AMLO_tokens[2]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['como',\n"," 'acordamos',\n"," 'vamos',\n"," 'a',\n"," 'presentar',\n"," 'un',\n"," 'informe',\n"," 'sobre',\n"," 'seguridad',\n"," 'lo',\n"," 'que',\n"," 'se',\n"," 'ha',\n"," 'venido',\n"," 'haciendo',\n"," 'los',\n"," 'resultados',\n"," 'en',\n"," 'este',\n"," 'tema',\n"," 'tan',\n"," 'complejo',\n"," 'y',\n"," 'tan',\n"," 'delicado',\n"," 'que',\n"," 'le',\n"," 'importa',\n"," 'mucho',\n"," 'mucho',\n"," 'a',\n"," 'los',\n"," 'mexicanos']"]},"metadata":{"tags":[]},"execution_count":468}]},{"cell_type":"code","metadata":{"id":"-qNeBBVQ63PR"},"source":["# Separo conjunto \n","X_train, X_val = train_test_split(AMLO_tokens, train_size = 0.8, test_size = 0.2, random_state = args.seed)\n","X_val, X_test  = train_test_split(X_val, train_size = 0.5, test_size = 0.5, random_state = args.seed)\n","\n","mymodel = language_model(X_train, X_val, vocabulary_size = 20000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVxunhMO7NHl","executionInfo":{"status":"ok","timestamp":1616215978286,"user_tz":360,"elapsed":24126,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"ca74a81f-9b7b-47ad-9dd4-db314073a5a9"},"source":["l = [[1/3, 1/3, 1/3], [0.4, 0.4, 0.2], [0.2, 0.4, 0.4], [0.5, 0.4, 0.1], [0.1, 0.4, 0.5]]\n","for lam in l :\n","  pp = mymodel.compute_perplexity(lam)\n","  print('Perplexity : ', np.exp(pp), 'with ', lam)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Perplexity :  440.7509166973431 with  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n","Perplexity :  502.9404754374155 with  [0.4, 0.4, 0.2]\n","Perplexity :  384.736931055309 with  [0.2, 0.4, 0.4]\n","Perplexity :  641.1499331336446 with  [0.5, 0.4, 0.1]\n","Perplexity :  352.43930812129275 with  [0.1, 0.4, 0.5]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x02ybSe-7haa"},"source":["import random \n","def dar_conferencia (pivot, model, lmda, size = 300) :\n","  final_sentence = pivot.copy()\n","  for i in range(size) :\n","    top, top_five = autocompletate_and_candidates(pivot, model, lmda)\n","    top = random.choice(top_five)\n","    # Actualizo secuancia\n","    pivot.pop(0)\n","    pivot.append(top[-1])\n","\n","    final_sentence += [top[-1]]\n","  return ' '.join(final_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"gV5M2h507yoU","executionInfo":{"status":"ok","timestamp":1616216356261,"user_tz":360,"elapsed":69544,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"bb24b081-c45b-48bd-f837-d11213f397cc"},"source":["#Prueba de longitud corta\n","\n","pivot = ['buenos','dias']\n","\n","dar_conferencia(pivot, mymodel, [0.1, 0.4, 0.5], size = 100)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'buenos dias con eso ya se va reduciendo anualmente vamos adelante a pemex en veracruz lamentablemente se tiene información de <unk> y de acuerdo con los trabajadores petroleros a los <unk> y no no sé bien <unk> a <unk> y de acuerdo sobre los trabajadores electricistas a trabajadores sindicalizados pidan refugio y ya estamos viendo todo tipo y de acuerdo también el presidente madero el país ahora que hay una situación difícil no vamos aquí a <unk> y de salud hospitales covid en todos los estados nacionales extranjeros deberán presentarnos el bienestar y se está trabajando todos estos programas precisamente zedillo y'"]},"metadata":{"tags":[]},"execution_count":506}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205},"id":"tYzlkB3o7-3w","executionInfo":{"status":"ok","timestamp":1616217631800,"user_tz":360,"elapsed":214507,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"be6ea37e-45c3-4bcf-9dd1-d9ce95df83b6"},"source":["pivot = ['buenos','dias']\n","\n","dar_conferencia(pivot, mymodel, [0.1, 0.4, 0.5])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'buenos dias si lo vemos aquí no había antes y eso fue <unk> a <unk> de la corrupción política migratoria el tema migratorio se puede probar técnica y que <unk> con <unk> de los ciudadanos <unk> que es <unk> y no no <unk> de las autoridades sanitarias que tiene <unk> que tenemos aquí también en general de seguridad ciudadana <unk> y de los medios nacionales e hijos no no hay impunidad y todo mundo se tiene <unk> a <unk> que tenemos todos los trabajadores mineros como el <unk> a los ciudadanos que <unk> que se tiene pensado para <unk> a la población afectada resaltando los <unk> a los mexicanos allá no todos los días aquí mismo se tiene <unk> y no sé yo soy partidario en eso es importante señalar es así la epidemia gracias gracias a <unk> y ya hay <unk> y ya hay <unk> no sólo se ha dicho nada sobre eso estamos aquí no sé nada <unk> nada de corrupción en el caso específico la gente que se tiene pensado que se va <unk> de las <unk> de que es importante también esto se puede probar es cierto eh esto tiene como son muchos más pero ya estamos atendiendo ya se terminó arriba para todos nosotros y todos todos los estados nacionales internacionales para garantizar las personas adultas cuenta quiénes participan sus hijos de que es el bienestar material pero sí no tienen razón no creo yo tengo que hacer esto también <unk> y de salud <unk> y de la ciudad juárez en esta situación lamentable y respuestas sobre los jóvenes estudiantes pobres no sé yo tengo aquí se va avanzando porque la información suficiente porque no sólo es un proceso penal <unk> de la defensa el país ahora más información sobre todo <unk> más que <unk> a la'"]},"metadata":{"tags":[]},"execution_count":519}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":137},"id":"ZGmr4X8AANC9","executionInfo":{"status":"ok","timestamp":1616217358173,"user_tz":360,"elapsed":308734,"user":{"displayName":"Esteban Reyes Saldaña","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz4gE7DISKQUP5iLzwc9HTNYF6LogJr8Qe6IdWBQ=s64","userId":"18048816362619501755"}},"outputId":"377a5e46-e243-4bf7-8d0c-04579687f70d"},"source":["pivot = ['hoy','vamos', 'a', 'hablar', 'de']\n","\n","dar_conferencia(pivot, mymodel, [0.1, 0.4, 0.5])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'hoy vamos a hablar de <unk> a los ciudadanos que se va comportando de <unk> y ya lo de <unk> a ver con las autoridades federales y de los <unk> de las <unk> a méxico y de <unk> no no hay un cambio profundo en eso sí calienta pues entonces eso sí no tienen para nosotros <unk> que tenemos ya no está bien gracias gracias por <unk> que tenemos nosotros que ver si hay alguna situación específica que sí sí se tiene contemplado el que tiene menos del 50 metros construcción total 663 mil <unk> y se puede decir más detalles que pudiera salir pronto para poder atender a la ley laboral de estados fronterizos el caso del país presidente amlo declaratoria de méxico aquí ya lo <unk> de que <unk> que tiene menos <unk> que tenemos una parte operativa que ver eso ya cambió porque es el país también para no violencia la secretaría del gobierno mexicano del petróleo que <unk> que tiene usted de eso es que es importante es el país <unk> todo todo completamente en eso es importante <unk> a ver aquí no había cupo entonces sí no tienen información presidente que <unk> y no no tenemos información suficiente pero también esto que tiene <unk> que es que hay corrupción ahí <unk> que <unk> y no <unk> no sólo se va dando cambios importantes y de la república también es <unk> a méxico y se puede decir más bajos del estado <unk> mil barriles de los ciudadanos <unk> que <unk> a la corrupción <unk> y se está dando seguimiento con ellos este año es decir <unk> pero también <unk> y que tiene <unk> y no sólo de la salud <unk> a la población <unk> <unk> a la corrupción <unk> que se ha ido creciendo de méxico también ayuda a conocer esto'"]},"metadata":{"tags":[]},"execution_count":518}]}]}